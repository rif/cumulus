<html>
<head>
<meta pid="AlvesThresholds-CameraReady" track="Research" title="Deriving Metric Thresholds from Benchmark Data" presenter="Tiago L. Alves" authors="Tiago L. Alves,Christiaan Ypma,Joost Visser" location="D1" date="16-09-2010 9:00"/>
<title>Deriving Metric Thresholds from Benchmark Data</title>
<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />
<link rel="stylesheet" href="../../css/paperstyle.css" type="text/css">
</head>
<body>
<h2>Deriving Metric Thresholds from Benchmark Data</h2>
<small class="alt"> Thursday, 16-09-2010 - 9:00 location: D1</small>
<hr>
<h4 class="alt">Authors: Tiago L. Alves, Christiaan Ypma and Joost Visser</h4>
<p>
A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metricâ€™s variability between systems and (ii) help focus on a reasonable percentage of the source code volume. Our method respects the distributions and scales of source code metrics, and it is resilient against outliers in metric values or system size. We applied our method to a benchmark of 100 object-oriented software systems, both proprietary and open-source, to derive thresholds for metrics included in the SIG maintainability model.

</p>
<br/>

</body>
</html>